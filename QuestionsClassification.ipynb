{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Classification Notebook\n",
    "@ author: Hatem Trigui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question        label\n",
      "0  How did serfdom develop in and then leave Russ...  DESC:manner\n",
      "1   What films featured the character Popeye Doyle ?  ENTY:cremat\n",
      "2  How can I find a list of celebrities ' real na...  DESC:manner\n",
      "3  What fowl grabs the spotlight after the Chines...  ENTY:animal\n",
      "4                    What is the full form of .com ?     ABBR:exp\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to load the data from a .txt file\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            # Split label and question\n",
    "            label, question = line.strip().split(' ', 1)\n",
    "            data.append({\"question\": question, \"label\": label})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load your training and test set\n",
    "train_set = load_data(\".\\data\\\\train_set5.txt\")\n",
    "test_set = load_data(\".\\data\\\\test_set.txt\")\n",
    "\n",
    "# Sample loaded data\n",
    "print(train_set.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\la7tim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\la7tim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\la7tim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 1. Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove special characters, numbers, and extra spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 3. Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # 5. Lemmatization (more accurate than stemming)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # 6. Reconstruct the text (optional if you want to keep the words as they are)\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the preprocessing to the entire dataset\n",
    "train_set['processed_question'] = train_set['question'].apply(preprocess_text)\n",
    "test_set['processed_question'] = test_set['question'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         label category specific_type\n",
      "0  DESC:manner     DESC        manner\n",
      "1  ENTY:cremat     ENTY        cremat\n",
      "2  DESC:manner     DESC        manner\n",
      "3  ENTY:animal     ENTY        animal\n",
      "4     ABBR:exp     ABBR           exp\n"
     ]
    }
   ],
   "source": [
    "# Split the label into category and specific type\n",
    "train_set[['category', 'specific_type']] = train_set['label'].str.split(':', expand=True)\n",
    "test_set[['category', 'specific_type']] = test_set['label'].str.split(':', expand=True)\n",
    "\n",
    "# Display a sample of the data\n",
    "print(train_set[['label', 'category', 'specific_type']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  category specific_type  category_encoded  specific_type_encoded\n",
      "0     DESC        manner                 1                     23\n",
      "1     ENTY        cremat                 2                      8\n",
      "2     DESC        manner                 1                     23\n",
      "3     ENTY        animal                 2                      1\n",
      "4     ABBR           exp                 0                     16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder for category and specific type\n",
    "category_encoder = LabelEncoder()\n",
    "specific_type_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the training labels\n",
    "train_set['category_encoded'] = category_encoder.fit_transform(train_set['category'])\n",
    "train_set['specific_type_encoded'] = specific_type_encoder.fit_transform(train_set['specific_type'])\n",
    "\n",
    "# Transform the test labels\n",
    "test_set['category_encoded'] = category_encoder.transform(test_set['category'])\n",
    "test_set['specific_type_encoded'] = specific_type_encoder.transform(test_set['specific_type'])\n",
    "\n",
    "# Check the encoding\n",
    "print(train_set[['category', 'specific_type', 'category_encoded', 'specific_type_encoded']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  combined_label  combined_label_encoded\n",
      "0    DESC_manner                       4\n",
      "1    ENTY_cremat                       9\n",
      "2    DESC_manner                       4\n",
      "3    ENTY_animal                       6\n",
      "4       ABBR_exp                       1\n"
     ]
    }
   ],
   "source": [
    "# Combine category and specific type into a single label\n",
    "train_set['combined_label'] = train_set['category'] + \"_\" + train_set['specific_type']\n",
    "test_set['combined_label'] = test_set['category'] + \"_\" + test_set['specific_type']\n",
    "\n",
    "# Encode the combined label\n",
    "combined_label_encoder = LabelEncoder()\n",
    "train_set['combined_label_encoded'] = combined_label_encoder.fit_transform(train_set['combined_label'])\n",
    "test_set['combined_label_encoded'] = combined_label_encoder.transform(test_set['combined_label'])\n",
    "\n",
    "# Check the encoded combined labels\n",
    "print(train_set[['combined_label', 'combined_label_encoded']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (X) are the questions (text)\n",
    "X_train = train_set['question']  # Replace 'question' with your actual column name for the questions\n",
    "X_test = test_set['question']  # Same for test set\n",
    "\n",
    "# Labels (y) are the encoded categories and specific types\n",
    "y_train_category = train_set['category_encoded']\n",
    "y_train_type = train_set['specific_type_encoded']\n",
    "\n",
    "y_test_category = test_set['category_encoded']\n",
    "y_test_type = test_set['specific_type_encoded']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tfidf shape: (5452, 8182)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Check the shape of the transformed data\n",
    "print(f\"X_train_tfidf shape: {X_train_tfidf.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train_split, X_val_split, y_train_split_category, y_val_split_category = train_test_split(\n",
    "    X_train_tfidf, y_train_category, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_split, X_val_split, y_train_split_type, y_val_split_type = train_test_split(\n",
    "    X_train_tfidf, y_train_type, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5452,)\n",
      "(1091, 8182)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_split.shape)\n",
    "print(X_val_split.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the TF-IDF transformation to the full training set before splitting\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Now, split the data into training and validation sets from the transformed data\n",
    "X_train_split, X_val_split, y_train_split_category, y_val_split_category = train_test_split(\n",
    "    X_train_tfidf, y_train_category, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_split, X_val_split, y_train_split_type, y_val_split_type = train_test_split(\n",
    "    X_train_tfidf, y_train_type, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5452, 8182)\n",
      "(500, 8182)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf.shape)  # Should print something like (number_of_samples, number_of_features)\n",
    "print(X_test_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the training data into training and validation sets for the category prediction\n",
    "X_train_split, X_val_split, y_train_split_category, y_val_split_category = train_test_split(\n",
    "    X_train_tfidf, y_train_category, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training data into training and validation sets for the type prediction\n",
    "X_train_split, X_val_split, y_train_split_type, y_val_split_type = train_test_split(\n",
    "    X_train_tfidf, y_train_type, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Prediction - Accuracy: 0.6599450045829515\n",
      "Category Prediction - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.52      0.69        23\n",
      "           1       0.57      0.67      0.62       227\n",
      "           2       0.61      0.64      0.62       246\n",
      "           3       0.65      0.72      0.68       240\n",
      "           4       0.80      0.69      0.74       166\n",
      "           5       0.76      0.60      0.67       189\n",
      "\n",
      "    accuracy                           0.66      1091\n",
      "   macro avg       0.73      0.64      0.67      1091\n",
      "weighted avg       0.67      0.66      0.66      1091\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "category_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model for category prediction\n",
    "category_model.fit(X_train_split, y_train_split_category)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred_category = category_model.predict(X_val_split)\n",
    "\n",
    "# Evaluate the performance on the validation set\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Accuracy and classification report for category prediction\n",
    "print(\"Category Prediction - Accuracy:\", accuracy_score(y_val_split_category, y_val_pred_category))\n",
    "print(\"Category Prediction - Classification Report:\")\n",
    "print(classification_report(y_val_split_category, y_val_pred_category))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
