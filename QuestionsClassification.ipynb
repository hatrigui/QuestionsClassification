{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hGpNUrfLoTu"
      },
      "source": [
        "# Question Classification Notebook Using Transformers approach\n",
        "@ author: Raby3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IradlyuaLoTx"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Co3jmIFMIZp",
        "outputId": "cec8c31a-b5b7-4d2a-efa8-a81eec9270b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.12.14)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC7rpULnLoTy",
        "outputId": "4c93db68-5e97-42fa-a39e-492486e5da14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gdown\n",
        "from contractions import fix\n",
        "\n",
        "# Download necessary NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2aJKJQpLoT0"
      },
      "source": [
        "## Data Loading and Parsing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#Loading Videos and aligned transcriptions (Annotaions)\n",
        "url = 'https://drive.google.com/drive/folders/1w_x5uJz6fVlVciSijUD7u_4xMNWFNKS-?usp=drive_link'\n",
        "gdown.download_folder(url, quiet=True)"
      ],
      "metadata": {
        "id": "J_0FPVfKL3z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAPtAbF-LoT0",
        "outputId": "9a08faa6-8572-4569-806a-0a215cfcb230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            question        label\n",
            "0  How did serfdom develop in and then leave Russ...  DESC:manner\n",
            "1   What films featured the character Popeye Doyle ?  ENTY:cremat\n",
            "2  How can I find a list of celebrities ' real na...  DESC:manner\n",
            "3  What fowl grabs the spotlight after the Chines...  ENTY:animal\n",
            "4                    What is the full form of .com ?     ABBR:exp\n"
          ]
        }
      ],
      "source": [
        "def load_data(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            # Split label and question\n",
        "            label, question = line.strip().split(' ', 1)\n",
        "            data.append({\"question\": question, \"label\": label})\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "train_set = load_data(\"/content/data/train_set5.txt\")\n",
        "test_set = load_data(\"/content/data/test_set.txt\")\n",
        "\n",
        "print(train_set.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vuk9f9MVLoT1"
      },
      "source": [
        "## Text Preprocessing\n",
        "\n",
        "1. Lowercased text.\n",
        "2. Removed special characters and numbers using regex.\n",
        "3. Tokenized text into words using nltk.\n",
        "4. Removed stopwords\n",
        "5. Applied lemmatization for text normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg1_M4BwLoT2"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "wh_words = {\"who\", \"what\", \"where\", \"when\", \"why\", \"how\", \"is\", \"are\", \"does\", \"do\", \"did\",\"was\"}\n",
        "custom_stop_words = stop_words - wh_words\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = fix(text)\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in custom_stop_words]\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "train_set['processed_question'] = train_set['question'].apply(preprocess_text)\n",
        "test_set['processed_question'] = test_set['question'].apply(preprocess_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Keep WH words##"
      ],
      "metadata": {
        "id": "-1wg6EmRTHNw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u28NN6hITGBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "jwJuhicULoT2",
        "outputId": "fe861a93-a41a-4ca4-f4ad-50c211753942"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                  how did serfdom develop leave russia\n",
              "1             what film featured character popeye doyle\n",
              "2                     how find list celebrity real name\n",
              "3          what fowl grab spotlight chinese year monkey\n",
              "4                                 what is full form com\n",
              "5          what contemptible scoundrel stole cork lunch\n",
              "6          what team did baseball st louis brown become\n",
              "7                             what is oldest profession\n",
              "8                                 what are liver enzyme\n",
              "9                 name scarfaced bounty hunter old west\n",
              "10                           when wa ozzy osbourne born\n",
              "11         why do heavier object travel downhill faster\n",
              "12                                  who wa pride yankee\n",
              "13                                    who killed gandhi\n",
              "14    what is considered costliest disaster insuranc...\n",
              "15                 what sprawling u state boast airport\n",
              "16      what did repealed amendment u constitution deal\n",
              "17        how many jew executed concentration camp wwii\n",
              "18                               what is nine inch nail\n",
              "19                       what is annotated bibliography\n",
              "Name: processed_question, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>processed_question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>how did serfdom develop leave russia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>what film featured character popeye doyle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>how find list celebrity real name</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what fowl grab spotlight chinese year monkey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what is full form com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>what contemptible scoundrel stole cork lunch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>what team did baseball st louis brown become</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>what is oldest profession</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>what are liver enzyme</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>name scarfaced bounty hunter old west</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>when wa ozzy osbourne born</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>why do heavier object travel downhill faster</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>who wa pride yankee</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>who killed gandhi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>what is considered costliest disaster insuranc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>what sprawling u state boast airport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>what did repealed amendment u constitution deal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>how many jew executed concentration camp wwii</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>what is nine inch nail</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>what is annotated bibliography</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "train_set['processed_question'].head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dlOG9aJLoT2"
      },
      "source": [
        "## Label Splitting:\n",
        "\n",
        "Split labels (e.g., DESC:manner) into category (DESC) and specific_type (manner), making the structure of your labels more granular."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQqzBGj4LoT3",
        "outputId": "85187f27-653a-4a2e-e789-3fe9870fb435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         label category specific_type\n",
            "0  DESC:manner     DESC        manner\n",
            "1  ENTY:cremat     ENTY        cremat\n",
            "2  DESC:manner     DESC        manner\n",
            "3  ENTY:animal     ENTY        animal\n",
            "4     ABBR:exp     ABBR           exp\n"
          ]
        }
      ],
      "source": [
        "train_set[['category', 'specific_type']] = train_set['label'].str.split(':', expand=True)\n",
        "test_set[['category', 'specific_type']] = test_set['label'].str.split(':', expand=True)\n",
        "\n",
        "print(train_set[['label', 'category', 'specific_type']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9coXxOzLoT3"
      },
      "source": [
        "## Encoding:\n",
        "\n",
        "Used LabelEncoder to convert textual labels (category, specific_type, and combined labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puvJHXAWLoT4",
        "outputId": "0fa9a8de-88f2-4d8a-a8b4-4ac293fc7b4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  category specific_type  category_encoded  specific_type_encoded\n",
            "0     DESC        manner                 1                     23\n",
            "1     ENTY        cremat                 2                      8\n",
            "2     DESC        manner                 1                     23\n",
            "3     ENTY        animal                 2                      1\n",
            "4     ABBR           exp                 0                     16\n"
          ]
        }
      ],
      "source": [
        "category_encoder = LabelEncoder()\n",
        "specific_type_encoder = LabelEncoder()\n",
        "\n",
        "train_set['category_encoded'] = category_encoder.fit_transform(train_set['category'])\n",
        "train_set['specific_type_encoded'] = specific_type_encoder.fit_transform(train_set['specific_type'])\n",
        "\n",
        "test_set['category_encoded'] = category_encoder.transform(test_set['category'])\n",
        "test_set['specific_type_encoded'] = specific_type_encoder.transform(test_set['specific_type'])\n",
        "\n",
        "print(train_set[['category', 'specific_type', 'category_encoded', 'specific_type_encoded']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whp7FmayLoT4",
        "outputId": "3e0a197b-8a8a-4fa7-c244-daa04dfc3b60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  combined_label  combined_label_encoded\n",
            "0    DESC_manner                       4\n",
            "1    ENTY_cremat                       9\n",
            "2    DESC_manner                       4\n",
            "3    ENTY_animal                       6\n",
            "4       ABBR_exp                       1\n"
          ]
        }
      ],
      "source": [
        "train_set['combined_label'] = train_set['category'] + \"_\" + train_set['specific_type']\n",
        "test_set['combined_label'] = test_set['category'] + \"_\" + test_set['specific_type']\n",
        "\n",
        "combined_label_encoder = LabelEncoder()\n",
        "train_set['combined_label_encoded'] = combined_label_encoder.fit_transform(train_set['combined_label'])\n",
        "test_set['combined_label_encoded'] = combined_label_encoder.transform(test_set['combined_label'])\n",
        "\n",
        "print(train_set[['combined_label', 'combined_label_encoded']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "Qf3bs0tRLoT4",
        "outputId": "2c3af1de-064d-465e-d73a-fb5d1d7da6c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            question        label  \\\n",
              "0  How did serfdom develop in and then leave Russ...  DESC:manner   \n",
              "1   What films featured the character Popeye Doyle ?  ENTY:cremat   \n",
              "2  How can I find a list of celebrities ' real na...  DESC:manner   \n",
              "3  What fowl grabs the spotlight after the Chines...  ENTY:animal   \n",
              "4                    What is the full form of .com ?     ABBR:exp   \n",
              "\n",
              "                             processed_question category specific_type  \\\n",
              "0          how did serfdom develop leave russia     DESC        manner   \n",
              "1     what film featured character popeye doyle     ENTY        cremat   \n",
              "2             how find list celebrity real name     DESC        manner   \n",
              "3  what fowl grab spotlight chinese year monkey     ENTY        animal   \n",
              "4                         what is full form com     ABBR           exp   \n",
              "\n",
              "   category_encoded  specific_type_encoded combined_label  \\\n",
              "0                 1                     23    DESC_manner   \n",
              "1                 2                      8    ENTY_cremat   \n",
              "2                 1                     23    DESC_manner   \n",
              "3                 2                      1    ENTY_animal   \n",
              "4                 0                     16       ABBR_exp   \n",
              "\n",
              "   combined_label_encoded  \n",
              "0                       4  \n",
              "1                       9  \n",
              "2                       4  \n",
              "3                       6  \n",
              "4                       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7dc33e0e-138b-4cc0-ac34-e17ea9c63c75\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>label</th>\n",
              "      <th>processed_question</th>\n",
              "      <th>category</th>\n",
              "      <th>specific_type</th>\n",
              "      <th>category_encoded</th>\n",
              "      <th>specific_type_encoded</th>\n",
              "      <th>combined_label</th>\n",
              "      <th>combined_label_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How did serfdom develop in and then leave Russ...</td>\n",
              "      <td>DESC:manner</td>\n",
              "      <td>how did serfdom develop leave russia</td>\n",
              "      <td>DESC</td>\n",
              "      <td>manner</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>DESC_manner</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What films featured the character Popeye Doyle ?</td>\n",
              "      <td>ENTY:cremat</td>\n",
              "      <td>what film featured character popeye doyle</td>\n",
              "      <td>ENTY</td>\n",
              "      <td>cremat</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>ENTY_cremat</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How can I find a list of celebrities ' real na...</td>\n",
              "      <td>DESC:manner</td>\n",
              "      <td>how find list celebrity real name</td>\n",
              "      <td>DESC</td>\n",
              "      <td>manner</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>DESC_manner</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
              "      <td>ENTY:animal</td>\n",
              "      <td>what fowl grab spotlight chinese year monkey</td>\n",
              "      <td>ENTY</td>\n",
              "      <td>animal</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>ENTY_animal</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the full form of .com ?</td>\n",
              "      <td>ABBR:exp</td>\n",
              "      <td>what is full form com</td>\n",
              "      <td>ABBR</td>\n",
              "      <td>exp</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>ABBR_exp</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7dc33e0e-138b-4cc0-ac34-e17ea9c63c75')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7dc33e0e-138b-4cc0-ac34-e17ea9c63c75 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7dc33e0e-138b-4cc0-ac34-e17ea9c63c75');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ffeee3f0-dd56-4316-8605-55566b95325b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ffeee3f0-dd56-4316-8605-55566b95325b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ffeee3f0-dd56-4316-8605-55566b95325b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_set",
              "summary": "{\n  \"name\": \"train_set\",\n  \"rows\": 5452,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5381,\n        \"samples\": [\n          \"What was Queen Victoria 's title regarding India ?\",\n          \"What is the origin of the proverb `` A stitch in time saves nine '' ?\",\n          \"What Caribbean island is northeast of Trinidad ?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"NUM:count\",\n          \"ENTY:word\",\n          \"ENTY:sport\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processed_question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5370,\n        \"samples\": [\n          \"how is correct say qigong\",\n          \"who appointed chair federal reserve\",\n          \"what is origin expression tout de suite\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"DESC\",\n          \"ENTY\",\n          \"LOC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"specific_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 47,\n        \"samples\": [\n          \"period\",\n          \"code\",\n          \"product\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category_encoded\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1,\n          2,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"specific_type_encoded\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 0,\n        \"max\": 46,\n        \"num_unique_values\": 47,\n        \"samples\": [\n          29,\n          4,\n          31\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combined_label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"NUM_count\",\n          \"ENTY_word\",\n          \"ENTY_sport\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combined_label_encoded\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13,\n        \"min\": 0,\n        \"max\": 49,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          38,\n          27,\n          21\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "train_set.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuD4bjejLoT5",
        "outputId": "815a75f8-7416-411f-e41b-6f7584227337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "category\n",
            "ENTY    1250\n",
            "HUM     1223\n",
            "DESC    1162\n",
            "NUM      896\n",
            "LOC      835\n",
            "ABBR      86\n",
            "Name: count, dtype: int64\n",
            "specific_type\n",
            "ind          962\n",
            "other        733\n",
            "def          421\n",
            "count        363\n",
            "desc         321\n",
            "manner       276\n",
            "date         218\n",
            "cremat       207\n",
            "reason       191\n",
            "gr           189\n",
            "country      155\n",
            "city         129\n",
            "animal       112\n",
            "food         103\n",
            "dismed       103\n",
            "termeq        93\n",
            "period        75\n",
            "money         71\n",
            "exp           70\n",
            "state         66\n",
            "sport         62\n",
            "event         56\n",
            "product       42\n",
            "substance     41\n",
            "color         40\n",
            "techmeth      38\n",
            "dist          34\n",
            "veh           27\n",
            "perc          27\n",
            "word          26\n",
            "title         25\n",
            "mount         21\n",
            "body          16\n",
            "abb           16\n",
            "lang          16\n",
            "plant         13\n",
            "volsize       13\n",
            "weight        11\n",
            "symbol        11\n",
            "instru        10\n",
            "letter         9\n",
            "code           9\n",
            "speed          9\n",
            "temp           8\n",
            "ord            6\n",
            "religion       4\n",
            "currency       4\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(train_set['category'].value_counts())\n",
        "print(train_set['specific_type'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Baw_BQoLoT5"
      },
      "source": [
        "## Observations\n",
        "1. **Categories**\n",
        "- The ENTY (Entity), HUM (Human), and DESC (Description) categories dominate the dataset.\n",
        "- ABBR (Abbreviation) is severely underrepresented with only 86 instances, making it a minority class.\n",
        "\n",
        "2. **Specific Types**\n",
        "The specific_type distribution is quite imbalanced:\n",
        "- ind, other, and def are the most common types.\n",
        "- Some specific types, like currency, religion, ord, and temp, have fewer than 10 instances.\n",
        "\n",
        "## Challenges\n",
        "1. **Class Imbalance:**\n",
        "Both category and specific_type have significant imbalances.\n",
        "Minority classes may lead to poor model performance for those classes.\n",
        "\n",
        "2. **Granularity:**\n",
        "Some specific types, such as techmeth and volsize, are too granular, which may increase the complexity of classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9stmGKlLoT6",
        "outputId": "4386c695-f754-4a96-fc76-9356e4856d6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category classes: ['ABBR' 'DESC' 'ENTY' 'HUM' 'LOC' 'NUM']\n",
            "Specific type classes: ['abb' 'animal' 'body' 'city' 'code' 'color' 'count' 'country' 'cremat'\n",
            " 'currency' 'date' 'def' 'desc' 'dismed' 'dist' 'event' 'exp' 'food' 'gr'\n",
            " 'ind' 'instru' 'lang' 'letter' 'manner' 'money' 'mount' 'ord' 'other'\n",
            " 'perc' 'period' 'plant' 'product' 'reason' 'religion' 'speed' 'sport'\n",
            " 'state' 'substance' 'symbol' 'techmeth' 'temp' 'termeq' 'title' 'veh'\n",
            " 'volsize' 'weight' 'word']\n",
            "Combined label classes: ['ABBR_abb' 'ABBR_exp' 'DESC_def' 'DESC_desc' 'DESC_manner' 'DESC_reason'\n",
            " 'ENTY_animal' 'ENTY_body' 'ENTY_color' 'ENTY_cremat' 'ENTY_currency'\n",
            " 'ENTY_dismed' 'ENTY_event' 'ENTY_food' 'ENTY_instru' 'ENTY_lang'\n",
            " 'ENTY_letter' 'ENTY_other' 'ENTY_plant' 'ENTY_product' 'ENTY_religion'\n",
            " 'ENTY_sport' 'ENTY_substance' 'ENTY_symbol' 'ENTY_techmeth' 'ENTY_termeq'\n",
            " 'ENTY_veh' 'ENTY_word' 'HUM_desc' 'HUM_gr' 'HUM_ind' 'HUM_title'\n",
            " 'LOC_city' 'LOC_country' 'LOC_mount' 'LOC_other' 'LOC_state' 'NUM_code'\n",
            " 'NUM_count' 'NUM_date' 'NUM_dist' 'NUM_money' 'NUM_ord' 'NUM_other'\n",
            " 'NUM_perc' 'NUM_period' 'NUM_speed' 'NUM_temp' 'NUM_volsize' 'NUM_weight']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Category classes: {category_encoder.classes_}\")\n",
        "print(f\"Specific type classes: {specific_type_encoder.classes_}\")\n",
        "print(f\"Combined label classes: {combined_label_encoder.classes_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUW8ULCWLoT6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define the Tokenizer and fit it on your training data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_set['processed_question'])\n",
        "\n",
        "# Convert the text data into sequences\n",
        "X_train_sequences = tokenizer.texts_to_sequences(train_set['processed_question'])\n",
        "X_test_sequences = tokenizer.texts_to_sequences(test_set['processed_question'])\n",
        "\n",
        "# Get the maximum sequence length (for padding purposes)\n",
        "max_sequence_length = max([len(seq) for seq in X_train_sequences])\n",
        "\n",
        "# Pad the sequences to make them all the same length\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCD4bNApLoT7"
      },
      "outputs": [],
      "source": [
        "# Extract the labels\n",
        "y_train = train_set['combined_label_encoded']\n",
        "y_test = test_set['combined_label_encoded']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3cdR-tVLoT7",
        "outputId": "79e25089-7d12-465d-d227-cc4ee4356f19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5452, 22)\n",
            "(500, 22)\n",
            "(5452,)\n",
            "(500,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train_padded.shape)  # Should print (num_train_samples, max_sequence_length)\n",
        "print(X_test_padded.shape)   # Should print (num_test_samples, max_sequence_length)\n",
        "print(y_train.shape)         # Should print (num_train_samples,)\n",
        "print(y_test.shape)          # Should print (num_test_samples,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chjvtsf4LoT8",
        "outputId": "b9d9f915-7d8a-4000-c99f-10fb39ff39e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "import numpy as np\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=X_train_padded.shape[1]))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(len(np.unique(y_train)), activation='softmax'))  # Multi-class classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orP2s51uLoT8"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hJ9l6eOLoT8",
        "outputId": "07661062-6c15-4b19-b2d3-3e1addbfd106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 87ms/step - accuracy: 0.1803 - loss: 3.3920 - val_accuracy: 0.1220 - val_loss: 3.3321\n",
            "Epoch 2/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 85ms/step - accuracy: 0.2623 - loss: 2.7987 - val_accuracy: 0.4100 - val_loss: 2.5892\n",
            "Epoch 3/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 89ms/step - accuracy: 0.4308 - loss: 2.0480 - val_accuracy: 0.4260 - val_loss: 2.1604\n",
            "Epoch 4/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 95ms/step - accuracy: 0.5177 - loss: 1.6988 - val_accuracy: 0.5020 - val_loss: 2.0167\n",
            "Epoch 5/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 93ms/step - accuracy: 0.6165 - loss: 1.3210 - val_accuracy: 0.5340 - val_loss: 2.0178\n",
            "Epoch 6/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 97ms/step - accuracy: 0.6560 - loss: 1.1904 - val_accuracy: 0.5360 - val_loss: 1.9687\n",
            "Epoch 7/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 94ms/step - accuracy: 0.7424 - loss: 0.9096 - val_accuracy: 0.5900 - val_loss: 1.9781\n",
            "Epoch 8/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 98ms/step - accuracy: 0.7844 - loss: 0.7899 - val_accuracy: 0.6120 - val_loss: 2.0685\n",
            "Epoch 9/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 93ms/step - accuracy: 0.8063 - loss: 0.6958 - val_accuracy: 0.6220 - val_loss: 2.0662\n",
            "Epoch 10/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 83ms/step - accuracy: 0.8257 - loss: 0.6142 - val_accuracy: 0.6280 - val_loss: 2.0149\n",
            "Epoch 11/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 86ms/step - accuracy: 0.8484 - loss: 0.5341 - val_accuracy: 0.6320 - val_loss: 2.1309\n",
            "Epoch 12/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 83ms/step - accuracy: 0.8649 - loss: 0.4820 - val_accuracy: 0.6260 - val_loss: 2.0605\n",
            "Epoch 13/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 94ms/step - accuracy: 0.8803 - loss: 0.4292 - val_accuracy: 0.6680 - val_loss: 2.0940\n",
            "Epoch 14/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 103ms/step - accuracy: 0.8913 - loss: 0.3767 - val_accuracy: 0.6460 - val_loss: 2.1384\n",
            "Epoch 15/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 106ms/step - accuracy: 0.9055 - loss: 0.3411 - val_accuracy: 0.6640 - val_loss: 2.1323\n",
            "Epoch 16/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 87ms/step - accuracy: 0.9112 - loss: 0.3160 - val_accuracy: 0.6520 - val_loss: 2.0126\n",
            "Epoch 17/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 85ms/step - accuracy: 0.9290 - loss: 0.2686 - val_accuracy: 0.6280 - val_loss: 2.2267\n",
            "Epoch 18/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 99ms/step - accuracy: 0.9255 - loss: 0.2538 - val_accuracy: 0.6400 - val_loss: 2.2455\n",
            "Epoch 19/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 84ms/step - accuracy: 0.9344 - loss: 0.2363 - val_accuracy: 0.6220 - val_loss: 2.2667\n",
            "Epoch 20/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 84ms/step - accuracy: 0.9412 - loss: 0.2041 - val_accuracy: 0.6340 - val_loss: 2.3234\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    X_train_padded,\n",
        "    y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_padded, y_test)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4Nk6qdnLoT9",
        "outputId": "1d432d94-dccc-4017-8f06-ea70e280893a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.6384 - loss: 2.2848\n",
            "Test Accuracy: 0.6340000033378601\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test_padded, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Zero Shot Classifiers##\n",
        "Large language models are zero-shot text classifiers"
      ],
      "metadata": {
        "id": "lLYZ43NIycwf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8p0tviQpycFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bert for Question Classification##\n",
        "Question and Answer Classification with Deep Contextualized Transformer\n",
        "Attention is all you need\n",
        "Attention-Based Transformer-BiGRU for\n",
        "Question Classification\n"
      ],
      "metadata": {
        "id": "xrWpRfVBxZBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import tqdm\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "67jrN5CnxL5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizer for Bert\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG69vJkb0Non",
        "outputId": "9b340417-8b23-4e18-b92d-a93fd62c97e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset\n",
        "class QuestionDataset(Dataset):\n",
        "    def __init__(self, questions, labels, max_len=64):\n",
        "        super(QuestionDataset, self).__init__()\n",
        "        self.questions = questions\n",
        "        self.labels = labels\n",
        "        self.max_len = max_len\n",
        "    def __getitem__(self, item):\n",
        "        question = self.questions[item]\n",
        "        label = self.labels[item]\n",
        "\n",
        "        # Tokenize question\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            question,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids = encoded_dict['input_ids'].squeeze()\n",
        "        attention_mask = encoded_dict['attention_mask'].squeeze()\n",
        "        return input_ids, attention_mask, torch.tensor(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)"
      ],
      "metadata": {
        "id": "jxCQE6og0XWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TuBj9QJq3bFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Modeling\n",
        "class BertQuestionClassification(nn.Module):\n",
        "    def __init__(self, n_classes, pretrained_name='bert-base-uncased'):\n",
        "        super(BertQuestionClassification, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(pretrained_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classification = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        out = self.bert(input_ids, attention_mask)\n",
        "        out = out.last_hidden_state[:, 0, :]\n",
        "        out = self.dropout(out)\n",
        "        out = self.classification(out)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "thI4SlTZ09oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building methods\n",
        "def category_to_index(csv_path='/content/Question_Classification_Dataset.csv'):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    label = df['Category0']\n",
        "    category = sorted(set(label.values))\n",
        "    return {word: index for index, word in enumerate(category)}\n",
        "def get_data(csv_path='/content/Question_Classification_Dataset.csv'):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    questions = list(df['Questions'])\n",
        "    category = category_to_index(csv_path=csv_path)\n",
        "    labels = [category[i] for i in df['Category0'].values]\n",
        "    return questions, labels"
      ],
      "metadata": {
        "id": "wWJMpjBs1Pqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "    # Set up device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load data\n",
        "    questions, labels = get_data()\n",
        "\n",
        "    # Split data\n",
        "    X_trainval, X_test, y_trainval, y_test = train_test_split(questions, labels, test_size=0.3, random_state=2021)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.4, random_state=2021)\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    BATCH_SIZE = 40\n",
        "    train_dataset = QuestionDataset(X_train, y_train)\n",
        "    val_dataset = QuestionDataset(X_val, y_val)\n",
        "    test_dataset = QuestionDataset(X_test, y_test)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=len(X_test))\n",
        "\n",
        "    # Initialize model and move it to the device\n",
        "    model = BertQuestionClassification(n_classes=6)\n",
        "    model = model.to(device)  # Move model to GPU\n",
        "\n",
        "    # Define optimizer and loss function\n",
        "    optimizer = Adam(model.parameters(), lr=2e-5)\n",
        "    loss_fn = CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    N_EPOCHS = 20\n",
        "    MODEL_SAVE_PATH = '/content/model.pth'\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_f1scores = []\n",
        "\n",
        "    for epoch in range(N_EPOCHS):\n",
        "        print(f\"Epoch {epoch + 1}/{N_EPOCHS}\")\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_batch_losses = []\n",
        "        for input_ids, attention_mask, y_train_batch in tqdm.tqdm(train_dataloader):\n",
        "            # Move data to GPU\n",
        "            input_ids, attention_mask, y_train_batch = input_ids.to(device), attention_mask.to(device), y_train_batch.to(device)\n",
        "\n",
        "            y_train_pred = model(input_ids, attention_mask)\n",
        "            loss = loss_fn(y_train_pred, y_train_batch)\n",
        "            train_batch_losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_batch_losses = []\n",
        "        val_batch_f1scores = []\n",
        "        with torch.no_grad():\n",
        "            for input_ids, attention_mask, y_val_batch in val_dataloader:\n",
        "                # Move data to GPU\n",
        "                input_ids, attention_mask, y_val_batch = input_ids.to(device), attention_mask.to(device), y_val_batch.to(device)\n",
        "\n",
        "                y_val_pred = model(input_ids, attention_mask)\n",
        "                loss = loss_fn(y_val_pred, y_val_batch)\n",
        "                val_batch_losses.append(loss.item())\n",
        "\n",
        "                # Compute F1 score\n",
        "                predictions = torch.argmax(torch.nn.functional.softmax(y_val_pred, dim=-1), dim=-1)\n",
        "                val_batch_f1scores.append(\n",
        "                    f1_score(predictions.cpu(), y_val_batch.cpu(), average='macro')\n",
        "                )\n",
        "\n",
        "        # Metrics\n",
        "        train_losses.append(sum(train_batch_losses) / len(train_batch_losses))\n",
        "        val_losses.append(sum(val_batch_losses) / len(val_batch_losses))\n",
        "        val_f1scores.append(sum(val_batch_f1scores) / len(val_batch_f1scores))\n",
        "\n",
        "        # Save model\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "\n",
        "        print(f\"Train Loss: {train_losses[-1]:.4f} | Validation Loss: {val_losses[-1]:.4f} | Validation F1: {val_f1scores[-1]:.4f}\")\n",
        "\n",
        "# Entry point\n",
        "if __name__ == '__main__':\n",
        "    train_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88vT22t02poT",
        "outputId": "47cb77fa-2e23-45d9-cc11-da9138b5cd4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:20<00:00,  2.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9383 | Validation Loss: 0.3817 | Validation F1: 0.8038\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:21<00:00,  2.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2553 | Validation Loss: 0.2542 | Validation F1: 0.8572\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:21<00:00,  2.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1035 | Validation Loss: 0.2591 | Validation F1: 0.9015\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:21<00:00,  2.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0519 | Validation Loss: 0.2691 | Validation F1: 0.9031\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:21<00:00,  2.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0302 | Validation Loss: 0.2553 | Validation F1: 0.9197\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:21<00:00,  2.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0214 | Validation Loss: 0.2856 | Validation F1: 0.9083\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:21<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0154 | Validation Loss: 0.2975 | Validation F1: 0.9244\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:21<00:00,  2.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0106 | Validation Loss: 0.2929 | Validation F1: 0.9379\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:21<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0141 | Validation Loss: 0.2888 | Validation F1: 0.9266\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:21<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0074 | Validation Loss: 0.3093 | Validation F1: 0.9085\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:21<00:00,  2.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0126 | Validation Loss: 0.3306 | Validation F1: 0.9205\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:21<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0125 | Validation Loss: 0.3508 | Validation F1: 0.9188\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:21<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0033 | Validation Loss: 0.3438 | Validation F1: 0.9268\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:21<00:00,  2.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0017 | Validation Loss: 0.3465 | Validation F1: 0.9285\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:21<00:00,  2.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0017 | Validation Loss: 0.3592 | Validation F1: 0.9254\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:21<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0016 | Validation Loss: 0.3541 | Validation F1: 0.9271\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:21<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0064 | Validation Loss: 0.3421 | Validation F1: 0.9200\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:21<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0015 | Validation Loss: 0.3473 | Validation F1: 0.9276\n",
            "Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:21<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0014 | Validation Loss: 0.3623 | Validation F1: 0.9183\n",
            "Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:21<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0257 | Validation Loss: 0.3423 | Validation F1: 0.9079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction\n"
      ],
      "metadata": {
        "id": "FwC5kuFUAVoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word2Vec - ElMo - Embeddings for classification"
      ],
      "metadata": {
        "id": "XP_ilMfqyGX8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u1ozWjwDyVGA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}