{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Classification Notebook\n",
    "@ author: Hatem Trigui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\la7tim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\la7tim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\la7tim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question        label\n",
      "0  How did serfdom develop in and then leave Russ...  DESC:manner\n",
      "1   What films featured the character Popeye Doyle ?  ENTY:cremat\n",
      "2  How can I find a list of celebrities ' real na...  DESC:manner\n",
      "3  What fowl grabs the spotlight after the Chines...  ENTY:animal\n",
      "4                    What is the full form of .com ?     ABBR:exp\n"
     ]
    }
   ],
   "source": [
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            # Split label and question\n",
    "            label, question = line.strip().split(' ', 1)\n",
    "            data.append({\"question\": question, \"label\": label})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "train_set = load_data(\".\\data\\\\train_set5.txt\")\n",
    "test_set = load_data(\".\\data\\\\test_set.txt\")\n",
    "\n",
    "print(train_set.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "1. Lowercased text.\n",
    "2. Removed special characters and numbers using regex.\n",
    "3. Tokenized text into words using nltk.\n",
    "4. Removed stopwords \n",
    "5. Applied lemmatization for text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "train_set['processed_question'] = train_set['question'].apply(preprocess_text)\n",
    "test_set['processed_question'] = test_set['question'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               serfdom develop leave russia\n",
       "1       film featured character popeye doyle\n",
       "2              find list celebrity real name\n",
       "3    fowl grab spotlight chinese year monkey\n",
       "4                              full form com\n",
       "Name: processed_question, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set['processed_question'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Splitting:\n",
    "\n",
    "Split labels (e.g., DESC:manner) into category (DESC) and specific_type (manner), making the structure of your labels more granular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         label category specific_type\n",
      "0  DESC:manner     DESC        manner\n",
      "1  ENTY:cremat     ENTY        cremat\n",
      "2  DESC:manner     DESC        manner\n",
      "3  ENTY:animal     ENTY        animal\n",
      "4     ABBR:exp     ABBR           exp\n"
     ]
    }
   ],
   "source": [
    "train_set[['category', 'specific_type']] = train_set['label'].str.split(':', expand=True)\n",
    "test_set[['category', 'specific_type']] = test_set['label'].str.split(':', expand=True)\n",
    "\n",
    "print(train_set[['label', 'category', 'specific_type']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding:\n",
    "\n",
    "Used LabelEncoder to convert textual labels (category, specific_type, and combined labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  category specific_type  category_encoded  specific_type_encoded\n",
      "0     DESC        manner                 1                     23\n",
      "1     ENTY        cremat                 2                      8\n",
      "2     DESC        manner                 1                     23\n",
      "3     ENTY        animal                 2                      1\n",
      "4     ABBR           exp                 0                     16\n"
     ]
    }
   ],
   "source": [
    "category_encoder = LabelEncoder()\n",
    "specific_type_encoder = LabelEncoder()\n",
    "\n",
    "train_set['category_encoded'] = category_encoder.fit_transform(train_set['category'])\n",
    "train_set['specific_type_encoded'] = specific_type_encoder.fit_transform(train_set['specific_type'])\n",
    "\n",
    "test_set['category_encoded'] = category_encoder.transform(test_set['category'])\n",
    "test_set['specific_type_encoded'] = specific_type_encoder.transform(test_set['specific_type'])\n",
    "\n",
    "print(train_set[['category', 'specific_type', 'category_encoded', 'specific_type_encoded']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  combined_label  combined_label_encoded\n",
      "0    DESC_manner                       4\n",
      "1    ENTY_cremat                       9\n",
      "2    DESC_manner                       4\n",
      "3    ENTY_animal                       6\n",
      "4       ABBR_exp                       1\n"
     ]
    }
   ],
   "source": [
    "train_set['combined_label'] = train_set['category'] + \"_\" + train_set['specific_type']\n",
    "test_set['combined_label'] = test_set['category'] + \"_\" + test_set['specific_type']\n",
    "\n",
    "combined_label_encoder = LabelEncoder()\n",
    "train_set['combined_label_encoded'] = combined_label_encoder.fit_transform(train_set['combined_label'])\n",
    "test_set['combined_label_encoded'] = combined_label_encoder.transform(test_set['combined_label'])\n",
    "\n",
    "print(train_set[['combined_label', 'combined_label_encoded']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_question</th>\n",
       "      <th>category</th>\n",
       "      <th>specific_type</th>\n",
       "      <th>category_encoded</th>\n",
       "      <th>specific_type_encoded</th>\n",
       "      <th>combined_label</th>\n",
       "      <th>combined_label_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "      <td>DESC:manner</td>\n",
       "      <td>serfdom develop leave russia</td>\n",
       "      <td>DESC</td>\n",
       "      <td>manner</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>DESC_manner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "      <td>ENTY:cremat</td>\n",
       "      <td>film featured character popeye doyle</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>cremat</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>ENTY_cremat</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I find a list of celebrities ' real na...</td>\n",
       "      <td>DESC:manner</td>\n",
       "      <td>find list celebrity real name</td>\n",
       "      <td>DESC</td>\n",
       "      <td>manner</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>DESC_manner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "      <td>ENTY:animal</td>\n",
       "      <td>fowl grab spotlight chinese year monkey</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>animal</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>ENTY_animal</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the full form of .com ?</td>\n",
       "      <td>ABBR:exp</td>\n",
       "      <td>full form com</td>\n",
       "      <td>ABBR</td>\n",
       "      <td>exp</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>ABBR_exp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question        label  \\\n",
       "0  How did serfdom develop in and then leave Russ...  DESC:manner   \n",
       "1   What films featured the character Popeye Doyle ?  ENTY:cremat   \n",
       "2  How can I find a list of celebrities ' real na...  DESC:manner   \n",
       "3  What fowl grabs the spotlight after the Chines...  ENTY:animal   \n",
       "4                    What is the full form of .com ?     ABBR:exp   \n",
       "\n",
       "                        processed_question category specific_type  \\\n",
       "0             serfdom develop leave russia     DESC        manner   \n",
       "1     film featured character popeye doyle     ENTY        cremat   \n",
       "2            find list celebrity real name     DESC        manner   \n",
       "3  fowl grab spotlight chinese year monkey     ENTY        animal   \n",
       "4                            full form com     ABBR           exp   \n",
       "\n",
       "   category_encoded  specific_type_encoded combined_label  \\\n",
       "0                 1                     23    DESC_manner   \n",
       "1                 2                      8    ENTY_cremat   \n",
       "2                 1                     23    DESC_manner   \n",
       "3                 2                      1    ENTY_animal   \n",
       "4                 0                     16       ABBR_exp   \n",
       "\n",
       "   combined_label_encoded  \n",
       "0                       4  \n",
       "1                       9  \n",
       "2                       4  \n",
       "3                       6  \n",
       "4                       1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "ENTY    1250\n",
      "HUM     1223\n",
      "DESC    1162\n",
      "NUM      896\n",
      "LOC      835\n",
      "ABBR      86\n",
      "Name: count, dtype: int64\n",
      "specific_type\n",
      "ind          962\n",
      "other        733\n",
      "def          421\n",
      "count        363\n",
      "desc         321\n",
      "manner       276\n",
      "date         218\n",
      "cremat       207\n",
      "reason       191\n",
      "gr           189\n",
      "country      155\n",
      "city         129\n",
      "animal       112\n",
      "food         103\n",
      "dismed       103\n",
      "termeq        93\n",
      "period        75\n",
      "money         71\n",
      "exp           70\n",
      "state         66\n",
      "sport         62\n",
      "event         56\n",
      "product       42\n",
      "substance     41\n",
      "color         40\n",
      "techmeth      38\n",
      "dist          34\n",
      "veh           27\n",
      "perc          27\n",
      "word          26\n",
      "title         25\n",
      "mount         21\n",
      "body          16\n",
      "abb           16\n",
      "lang          16\n",
      "plant         13\n",
      "volsize       13\n",
      "weight        11\n",
      "symbol        11\n",
      "instru        10\n",
      "letter         9\n",
      "code           9\n",
      "speed          9\n",
      "temp           8\n",
      "ord            6\n",
      "religion       4\n",
      "currency       4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_set['category'].value_counts())\n",
    "print(train_set['specific_type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "1. **Categories**\n",
    "- The ENTY (Entity), HUM (Human), and DESC (Description) categories dominate the dataset.\n",
    "- ABBR (Abbreviation) is severely underrepresented with only 86 instances, making it a minority class.\n",
    "\n",
    "2. **Specific Types**\n",
    "The specific_type distribution is quite imbalanced:\n",
    "- ind, other, and def are the most common types.\n",
    "- Some specific types, like currency, religion, ord, and temp, have fewer than 10 instances.\n",
    "\n",
    "## Challenges\n",
    "1. **Class Imbalance:**\n",
    "Both category and specific_type have significant imbalances.\n",
    "Minority classes may lead to poor model performance for those classes.\n",
    "\n",
    "2. **Granularity:**\n",
    "Some specific types, such as techmeth and volsize, are too granular, which may increase the complexity of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category classes: ['ABBR' 'DESC' 'ENTY' 'HUM' 'LOC' 'NUM']\n",
      "Specific type classes: ['abb' 'animal' 'body' 'city' 'code' 'color' 'count' 'country' 'cremat'\n",
      " 'currency' 'date' 'def' 'desc' 'dismed' 'dist' 'event' 'exp' 'food' 'gr'\n",
      " 'ind' 'instru' 'lang' 'letter' 'manner' 'money' 'mount' 'ord' 'other'\n",
      " 'perc' 'period' 'plant' 'product' 'reason' 'religion' 'speed' 'sport'\n",
      " 'state' 'substance' 'symbol' 'techmeth' 'temp' 'termeq' 'title' 'veh'\n",
      " 'volsize' 'weight' 'word']\n",
      "Combined label classes: ['ABBR_abb' 'ABBR_exp' 'DESC_def' 'DESC_desc' 'DESC_manner' 'DESC_reason'\n",
      " 'ENTY_animal' 'ENTY_body' 'ENTY_color' 'ENTY_cremat' 'ENTY_currency'\n",
      " 'ENTY_dismed' 'ENTY_event' 'ENTY_food' 'ENTY_instru' 'ENTY_lang'\n",
      " 'ENTY_letter' 'ENTY_other' 'ENTY_plant' 'ENTY_product' 'ENTY_religion'\n",
      " 'ENTY_sport' 'ENTY_substance' 'ENTY_symbol' 'ENTY_techmeth' 'ENTY_termeq'\n",
      " 'ENTY_veh' 'ENTY_word' 'HUM_desc' 'HUM_gr' 'HUM_ind' 'HUM_title'\n",
      " 'LOC_city' 'LOC_country' 'LOC_mount' 'LOC_other' 'LOC_state' 'NUM_code'\n",
      " 'NUM_count' 'NUM_date' 'NUM_dist' 'NUM_money' 'NUM_ord' 'NUM_other'\n",
      " 'NUM_perc' 'NUM_period' 'NUM_speed' 'NUM_temp' 'NUM_volsize' 'NUM_weight']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Category classes: {category_encoder.classes_}\")\n",
    "print(f\"Specific type classes: {specific_type_encoder.classes_}\")\n",
    "print(f\"Combined label classes: {combined_label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define the Tokenizer and fit it on your training data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_set['processed_question'])  \n",
    "\n",
    "# Convert the text data into sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(train_set['processed_question'])\n",
    "X_test_sequences = tokenizer.texts_to_sequences(test_set['processed_question'])\n",
    "\n",
    "# Get the maximum sequence length (for padding purposes)\n",
    "max_sequence_length = max([len(seq) for seq in X_train_sequences])\n",
    "\n",
    "# Pad the sequences to make them all the same length\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the labels\n",
    "y_train = train_set['combined_label_encoded']\n",
    "y_test = test_set['combined_label_encoded']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5452, 18)\n",
      "(500, 18)\n",
      "(5452,)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_padded.shape)  # Should print (num_train_samples, max_sequence_length)\n",
    "print(X_test_padded.shape)   # Should print (num_test_samples, max_sequence_length)\n",
    "print(y_train.shape)         # Should print (num_train_samples,)\n",
    "print(y_test.shape)          # Should print (num_test_samples,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\la7tim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "import numpy as np\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=X_train_padded.shape[1]))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(np.unique(y_train)), activation='softmax'))  # Multi-class classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6659 - loss: 1.1199 - val_accuracy: 0.4220 - val_loss: 3.0255\n",
      "Epoch 2/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7251 - loss: 0.9628 - val_accuracy: 0.4000 - val_loss: 3.0658\n",
      "Epoch 3/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7532 - loss: 0.8297 - val_accuracy: 0.4320 - val_loss: 3.0505\n",
      "Epoch 4/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8040 - loss: 0.7116 - val_accuracy: 0.4360 - val_loss: 3.1212\n",
      "Epoch 5/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.8367 - loss: 0.6110 - val_accuracy: 0.4260 - val_loss: 3.2029\n",
      "Epoch 6/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8619 - loss: 0.5025 - val_accuracy: 0.4560 - val_loss: 3.0820\n",
      "Epoch 7/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.8800 - loss: 0.4433 - val_accuracy: 0.4840 - val_loss: 3.1307\n",
      "Epoch 8/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8915 - loss: 0.3964 - val_accuracy: 0.4840 - val_loss: 3.2768\n",
      "Epoch 9/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9032 - loss: 0.3594 - val_accuracy: 0.4740 - val_loss: 3.3635\n",
      "Epoch 10/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9096 - loss: 0.3291 - val_accuracy: 0.4420 - val_loss: 3.4451\n",
      "Epoch 11/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9209 - loss: 0.2825 - val_accuracy: 0.4860 - val_loss: 3.4291\n",
      "Epoch 12/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.9322 - loss: 0.2508 - val_accuracy: 0.4920 - val_loss: 3.4093\n",
      "Epoch 13/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9395 - loss: 0.2221 - val_accuracy: 0.4760 - val_loss: 3.6752\n",
      "Epoch 14/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9365 - loss: 0.2443 - val_accuracy: 0.5080 - val_loss: 3.5547\n",
      "Epoch 15/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9406 - loss: 0.2135 - val_accuracy: 0.5120 - val_loss: 3.6445\n",
      "Epoch 16/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9480 - loss: 0.1889 - val_accuracy: 0.4840 - val_loss: 3.6285\n",
      "Epoch 17/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9645 - loss: 0.1501 - val_accuracy: 0.5120 - val_loss: 3.6581\n",
      "Epoch 18/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.9621 - loss: 0.1423 - val_accuracy: 0.5020 - val_loss: 3.7553\n",
      "Epoch 19/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.9618 - loss: 0.1421 - val_accuracy: 0.5140 - val_loss: 3.9209\n",
      "Epoch 20/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9673 - loss: 0.1247 - val_accuracy: 0.4880 - val_loss: 3.9695\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train_padded, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_test_padded, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4549 - loss: 4.0337\n",
      "Test Accuracy: 0.4880000054836273\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
